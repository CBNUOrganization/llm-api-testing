from langchain_community.chat_models import ChatOllama
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Initialize the Ollama model with multilingual support and streaming enabled
llm = ChatOllama(
    model="gemma3",  # Ensure you have this model in Ollama
    base_url="http://10.255.78.58:9001",  # Change if using a remote server
    streaming=True,  # Enable streaming
    callbacks=[StreamingStdOutCallbackHandler()]  # Print output as it streams
)

# Define multilingual conversation examples
multilingual_messages = [
    ("English", "Tell me an interesting fact about space."),
    ("French", "Parle-moi d'un fait intÃ©ressant sur l'intelligence artificielle."),
    ("Spanish", "Dime algo interesante sobre la computaciÃ³n cuÃ¡ntica."),
    ("Korean", "ì–‘ì ì»´í“¨íŒ…ì— ëŒ€í•´ í¥ë¯¸ë¡œìš´ ì‚¬ì‹¤ì„ ì•Œë ¤ì£¼ì„¸ìš”."),
    ("Japanese", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦é¢ç™½ã„äº‹å®Ÿã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"),
    ("German", "ErzÃ¤hl mir eine interessante Tatsache Ã¼ber Quantenphysik."),
    ("Chinese", "è¯·å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æœ‰è¶£äº‹å®ã€‚")
]

# Loop through different languages
for lang, user_input in multilingual_messages:
    print(f"\nğŸŒ Language: {lang}")
    
    messages = [
        SystemMessage(content="You are a multilingual AI assistant."),
        HumanMessage(content=user_input)
    ]
    
    # Invoke the model and stream the response
    response = llm.invoke(messages)
    
    # Print a separator
    print("\nFull AI Response:", response.content)
